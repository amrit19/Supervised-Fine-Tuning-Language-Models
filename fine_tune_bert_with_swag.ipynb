{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional**\n",
    "\n",
    "Run the first cell if you want to interact with huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEALING WITH DATA FIRST**\n",
    "\n",
    "The first step is to import the data and preprocess it to a format that can be used by the model\n",
    "\n",
    "We use the Situations With Adversarial Generations (SWAG) dataset - https://rowanzellers.com/swag/ \n",
    "\n",
    "Here we import the dataset from the Datasets library of hugging face. \n",
    "\n",
    "Use 'pip install datasets' to install the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swag = load_dataset(\"swag\", \"regular\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOKENIZE DATA**\n",
    "\n",
    "We have to convert the dataset to a format that a Language Model can understand. Very similar to how we learn grammar, a language model has its own grammar and vocabulary based on the architecture.\n",
    "\n",
    "Since we are using BERT, we will use the Tokenizer of bert-base-uncased.\n",
    "\n",
    "I find it easy to use the transformer library of Hugging Face where they provide AutoTokenizer function that will automatically pick up the tokenizer depending on the model_id we choose. \n",
    "\n",
    "Use 'pip install transformers' to install the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>**WARNING** </font>\n",
    "\n",
    "If you get a warning that PyTorch/TensorFlow is not installed in your system, first install the cuda supported version of them.\n",
    "\n",
    "I have used pytorch here. One way to install pytorch would be to open the anaconda prompt and do the following: -\n",
    "\n",
    "conda create -n [enter your virtual environment name here] python=[enter the version of python you want to create your virtual environment for]\n",
    "\n",
    "conda activate [virtual env name]\n",
    "\n",
    "Install pytorch with cuda support (**very important if you have gpu and want to use it**) - https://pytorch.org/get-started/locally/\n",
    "\n",
    "conda deactivate (To exit the virtual environment)\n",
    "\n",
    "Use the virtual env in your IDE/Terminal for the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelForMultipleChoice.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HOW DOES THE DATASET LOOK AND HOW TO PREPROCESS?**\n",
    "\n",
    "If you have run the above cell, you will see that the swag dataset has several id's, startphrase, sent1, sent2, ending0, ending1, ending2, ending3, and label.\n",
    "\n",
    "This is where we have to be careful, depending on the task we have to preprocess our data. The problem that I want to solve is to fine tune BERT to predict an ending for sent2. The way I want to train BERT is the following: \n",
    "\n",
    "1) Input - sent1+sent2+ending_i , sent1+sent2+ending_j, we do this for all pair of endings but manually make sure that the correct answer exists in the pair.\n",
    "\n",
    "2) Loss - We then calculate loss between the prediction of BERT and the true label and update gradients.\n",
    "\n",
    "So we preprocess the dataset accordingly as follows: \n",
    "\n",
    "I have taken the preprocess function directly from Hugging Face - https://huggingface.co/docs/transformers/en/tasks/multiple_choice \n",
    "\n",
    "Note that it will work with BERT based architecture but you might need to check for other architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer):\n",
    "    ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n",
    "    \n",
    "    first_sentences = []\n",
    "    for context in examples[\"sent1\"]:\n",
    "        current_first_sentences = [context] * 4\n",
    "        first_sentences.append(current_first_sentences)\n",
    "    \n",
    "    question_headers = examples[\"sent2\"]\n",
    "    \n",
    "    second_sentences = []\n",
    "    for i, header in enumerate(question_headers):\n",
    "        current_second_sentences = []\n",
    "        for end in ending_names:\n",
    "            current_second_sentences.append(f\"{header} {examples[end][i]}\")\n",
    "        second_sentences.append(current_second_sentences)\n",
    "\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    \n",
    "    result = {}\n",
    "    for keys, values in tokenized_examples.items():\n",
    "        split_values = []\n",
    "        for i in range(0, len(values), 4):\n",
    "            split_values.append(values[i : i + 4])\n",
    "        result[keys] = split_values\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_swag = swag.map(lambda examples: preprocess_function(examples, tokenizer), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_swag[\"train\"].format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would notice additional fields of 'input_ids', 'token_type_ids', and 'attention_mask', which denotes the dataset has been tokenized. To know what each of these id's mean refer - https://huggingface.co/docs/transformers/en/glossary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face Transformers doesn’t have a data collator for multiple choice, so you’ll need to adapt the DataCollatorWithPadding to create a batch of examples. It’s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n",
    "\n",
    "DataCollatorForMultipleChoice flattens all the model inputs, applies padding, and then unflattens the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decide the metric you want to track while training**\n",
    "\n",
    "I have decided to go with accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING LOOP**\n",
    "\n",
    "I am using the TrainingArguments and Trainer provided by HuggingFace. This is optimized to run on models present in Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"fine-tuned-bert-base-uncased-swag\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    "    hub_token=\"\"#Enter your hub token here\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_swag[\"train\"],\n",
    "    eval_dataset=tokenized_swag[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the model to local instead of hugging face hub**\n",
    "Set push_to_hub=False in the TrainingArguments before training and run the cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"bert-swag-trained\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
