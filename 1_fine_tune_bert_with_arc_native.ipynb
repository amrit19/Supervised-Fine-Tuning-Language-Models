{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ARC dataset\n",
    "arc_dataset = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('amritpuhan/fine-tuned-bert-base-uncased-swag', token=\"\")\n",
    "model = AutoModelForMultipleChoice.from_pretrained('amritpuhan/fine-tuned-bert-base-uncased-swag', token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(label):\n",
    "    if label.isdigit():  # Check if the label is a digit\n",
    "        return int(label) - 1\n",
    "    else:  # Assume the label is a letter (A, B, C, D)\n",
    "        return ord(label) - ord('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_arc_function(examples):\n",
    "    # Unpack questions and choices\n",
    "    questions = examples[\"question\"]\n",
    "    choices = examples['choices']\n",
    "\n",
    "    # Prepare first and second sentences\n",
    "\n",
    "    first_sentences = []\n",
    "    second_sentences = []\n",
    "\n",
    "    # Prepare labels array if you need to handle labels dynamically as well\n",
    "    labels = []  \n",
    "\n",
    "    # Number of choices can vary\n",
    "    num_choices_per_question = []\n",
    "\n",
    "    for i, (question, choice_dict) in enumerate(zip(questions, choices)):\n",
    "        num_choices = len(choice_dict['text'])\n",
    "        num_choices_per_question.append(num_choices)\n",
    "        \n",
    "        # Repeat the question for each choice\n",
    "        first_sentences.extend([question] * num_choices)\n",
    "        \n",
    "        # Extend second sentences with each choice\n",
    "        second_sentences.extend(choice_dict['text'])\n",
    "\n",
    "        # If you're handling labels, adapt this part to your data structure\n",
    "        labels.append(convert_label(examples['answerKey'][i]))\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True, return_tensors='pt', padding=True)\n",
    "\n",
    "    # Un-flatten the tokenized outputs to maintain structure [number of examples, number of choices per example]\n",
    "    tokenized_outputs = {key: [] for key in tokenized_examples.keys()}\n",
    "    index = 0\n",
    "    for count in num_choices_per_question:\n",
    "        for key in tokenized_examples.keys():\n",
    "            tokenized_outputs[key].append(tokenized_examples[key][index:index + count])\n",
    "        index += count\n",
    "\n",
    "    # If using labels, make sure to format them here too\n",
    "    tokenized_outputs['labels'] = labels\n",
    "\n",
    "    return tokenized_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and prepare dataset\n",
    "#tokenized_arc = arc_dataset.map(lambda examples: preprocess_arc_function(examples, tokenizer), batched=True)\n",
    "tokenized_arc = arc_dataset.map(preprocess_arc_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_arc['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_arc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]  # Extract labels from features\n",
    "\n",
    "        flattened_features = []\n",
    "        num_choices_per_feature=[]\n",
    "        for feature in features:\n",
    "            # Determine the number of choices for the current question\n",
    "            num_choices = len(feature[\"input_ids\"])  # Assuming 'input_ids' represents the number of choices\n",
    "            num_choices_per_feature.append(num_choices)\n",
    "            # Iterate over each choice for the current feature\n",
    "            for i in range(num_choices):\n",
    "                # Create a dictionary for the current choice\n",
    "                choice_dict = {}\n",
    "\n",
    "                # Iterate over each key in the feature, excluding 'labels'\n",
    "                for key in feature:\n",
    "                    if key != 'labels' and key != 'id' and key != 'question' and key != 'choices' and key != 'answerKey':\n",
    "                        # Add the data for the current choice to the choice_dict\n",
    "                        choice_dict[key] = feature[key][i]\n",
    "\n",
    "                # Append the dictionary for the current choice to the flattened_features list\n",
    "                flattened_features.append(choice_dict)\n",
    "\n",
    "        # Pad the flattened features\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Reorganize padded data back to their respective feature structures\n",
    "        new_batch = {key: [] for key in batch.keys()}\n",
    "        current_index = 0\n",
    "        for num_choices in num_choices_per_feature:\n",
    "            for key in batch.keys():\n",
    "                new_batch[key].append(batch[key][current_index:current_index + num_choices])\n",
    "            current_index += num_choices\n",
    "\n",
    "        # Convert list of tensors back to tensor for each key\n",
    "        # This needs to handle variable sizes, so we use padding or similar approaches as required\n",
    "        for key in new_batch.keys():\n",
    "            new_batch[key] = torch.nn.utils.rnn.pad_sequence(new_batch[key], batch_first=True)\n",
    "\n",
    "        # Add back labels\n",
    "        new_batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return new_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming DataCollatorForMultipleChoice and ARC_Dataset are already defined\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)  # Move model to the appropriate device\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(tokenized_arc[\"train\"], batch_size=8, shuffle=True, collate_fn=DataCollatorForMultipleChoice(tokenizer))\n",
    "val_loader = DataLoader(tokenized_arc[\"validation\"], batch_size=8, shuffle=False, collate_fn=DataCollatorForMultipleChoice(tokenizer))\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Define a simple accuracy metric for evaluation\n",
    "def compute_accuracy(predictions, labels):\n",
    "    return (predictions == labels).float().mean()\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients to avoid explosion\n",
    "        optimizer.step()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        accuracy = compute_accuracy(predictions, labels)\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "    return total_loss / len(dataloader), total_accuracy / len(dataloader)\n",
    "\n",
    "\n",
    "# Evaluation loop\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    # Wrap dataloader with tqdm for a progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            accuracy = compute_accuracy(predictions, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy.item()\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': loss.item(), 'acc': accuracy.item()})\n",
    "    \n",
    "    return total_loss / len(dataloader), total_accuracy / len(dataloader)\n",
    "\n",
    "torch.cuda.empty_cache()  # Assuming your model has a reset_parameters method defined\n",
    "\n",
    "# Run the training and validation cycles\n",
    "for epoch in range(3):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
